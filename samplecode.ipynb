{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen AI Tutorial - Code snippets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import OpenAI key**\n",
    "\n",
    "Create a keys.py file  \n",
    "Add an entry that looks like this in the file  \n",
    "openai_api_key = \"Your OPENAI API key goes here\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from api_keys import openai_api_key\n",
    "# , serpapi_api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = serpapi_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Python libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-core\n",
    "%pip install -qU langchain-community\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain-google-genai\n",
    "%pip install -qU langchain\n",
    "\n",
    "%pip install -qU faiss-cpu\n",
    "%pip install -qU hdbcli\n",
    "%pip install -qU google-search-results\n",
    "%pip install -qU numexpr\n",
    "%pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Use ChatPromptTemplate to tell a joke about any topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import required classes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize llm, create prompt, invoke**\n",
    "\n",
    "System message sets the context and needs to be the first message - I guess you can skip it entirely as well  \n",
    "Then you can have alternate human and ai messages - to maintain the conversation  \n",
    "\n",
    "Note:  The output is an AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABSOLUTELY! WHY DID THE CAT SIT ON THE COMPUTER? \\n\\nBECAUSE IT WANTED TO KEEP AN EYE ON THE MOUSE! üê±üíª'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a world famous comedian\"),\n",
    "        (\"human\", \"Hi - what's up !\"),\n",
    "        (\"ai\", \"Doing great...  And you ?\"),\n",
    "        (\"human\", \"Meh...  Can you tell me a joke about {topic} to uplift my spirits\")\n",
    "    ])\n",
    "\n",
    "chain = chat_prompt_template | chat_llm | output_parser | RunnableLambda(lambda x: x.upper())\n",
    "chain.invoke({\"topic\": \"cats\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Create a fictitious product with the following JSON structure - name, description, price, rating  \n",
    "Output needs to be dict (not a string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import required classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create required structure, add it to chain__\n",
    "\n",
    "Create class with required structure  \n",
    "Initialize JsonOutputParser with the structure  \n",
    "\n",
    "Note:  The output is a dict  \n",
    "\n",
    "Bonus:  See if you can create 5 products...  Output needs to be a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(BaseModel):\n",
    "    name: str = Field(description=\"Name of product\")\n",
    "    description: str = Field(description=\"Description of product\")\n",
    "    price: float = Field(description=\"Price of the product\")\n",
    "    rating: int = Field(description=\"Rating of the product from 1 to 5\")\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful AI assistant.  \\nFormatting Instructions: {format_instructions}\"),\n",
    "        (\"human\", \"Can you create a fictitious product for the following category: {category}\")\n",
    "])\n",
    "                                     \n",
    "\n",
    "chain = chat_prompt_template | chat_llm | output_parser\n",
    "chain.invoke({\n",
    "    \"category\": \"Electronics\",\n",
    "    \"format_instructions\": output_parser.get_format_instructions()\n",
    "    })\n",
    "\n",
    "# result = chain.invoke({\n",
    "#     \"category\": \"Electronics\",\n",
    "#     \"format_instructions\": output_parser.get_format_instructions()\n",
    "#     })\n",
    "# print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inventory(BaseModel):\n",
    "    products: list[Product] = Field(description=\"This is the list of products\")\n",
    "    \n",
    "output_parser = JsonOutputParser(pydantic_object=Inventory)\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful AI assistant.  \\nFormatting Instructions: {format_instructions}\"),\n",
    "        (\"human\", \"Can you create 5 fictitious products for the following category: {category}\")\n",
    "])\n",
    "                                     \n",
    "\n",
    "chain = chat_prompt_template | chat_llm | output_parser\n",
    "chain.invoke({\n",
    "    \"category\": \"Electronics\",\n",
    "    \"format_instructions\": output_parser.get_format_instructions()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Create a summary of a long text - assume the long text will exceed token size limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import required classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load pdf file, use map_reduce method__\n",
    "\n",
    "Read up on how map_reduce summarization works  \n",
    "You can also have your own prompts - instead of using the built-in prompts like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"my_man_jeeves_story1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "summarize_chain = load_summarize_chain(llm=chat_llm, chain_type=\"map_reduce\")\n",
    "\n",
    "summary = summarize_chain.invoke(documents)\n",
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Implement RAG (use any vector DB of your choice) to query from some large pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import required classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Embeddings, get relevant emebddings for query, invoke llm__\n",
    "\n",
    "Create Embeddings using any vector database - we are using FAISS  \n",
    "Search for relevant embeddings for your query  \n",
    "Invoke llm with your relevant context and query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "loader = PyPDFLoader(\"my_man_jeeves_story1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "db = FAISS.from_documents(documents, embeddings_model)\n",
    "\n",
    "query = \"Who is Jeeves ?\"\n",
    "docs = db.similarity_search(query, k=4)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "chain = chat_llm | output_parser\n",
    "chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Use Agents to find out the number of Olympic medals won by United States (or any country) in 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import required classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load all the tools, initialize agent, submit prompt__\n",
    "\n",
    "Load all the tools that you think you might need - Typically you will load more than 1 tool here  \n",
    "Initialize the agent  \n",
    "Submit the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=chat_llm)\n",
    "agent = initialize_agent(toolkit, chat_llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)\n",
    "\n",
    "response = agent({\"input\":\"How many gold medals did US win in 2024 Olympics ?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
